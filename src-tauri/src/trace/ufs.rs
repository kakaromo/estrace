use std::collections::{BTreeMap, HashMap};
use std::fs::{create_dir_all, File};
use std::path::PathBuf;
use std::sync::Arc;

use arrow::array::{ArrayRef, BooleanArray, Float64Array, StringArray, UInt32Array, UInt64Array};
use arrow::datatypes::{DataType, Field, Schema};
use arrow::record_batch::RecordBatch;
use arrow::temporal_conversions::MILLISECONDS;
use parquet::arrow::ArrowWriter;
use tauri::Emitter;

use crate::trace::filter::{filter_ufs_data};
use crate::trace::utils::{
    calculate_statistics, create_range_key, initialize_ranges, parse_time_to_ms,
};
use crate::trace::{
    ContinuityCount, ContinuityStats, LatencyStat, LatencyStats, LatencyValue, SizeStats,
    TotalContinuity, TraceStats, UFS,
};

// UFS ë ˆì´í„´ì‹œ í†µê³„ ë¶„ì„ì„ ìœ„í•œ ë§¤ê°œë³€ìˆ˜ êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
pub struct UfsLatencyStatsParams {
    pub logname: String,
    pub column: String,
    pub zoom_column: String,
    pub time_from: Option<f64>,
    pub time_to: Option<f64>,
    pub col_from: Option<f64>,
    pub col_to: Option<f64>,
    pub thresholds: Vec<String>,
}

// UFS í¬ê¸° í†µê³„ ë¶„ì„ì„ ìœ„í•œ ë§¤ê°œë³€ìˆ˜ êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
pub struct UfsSizeStatsParams {
    pub logname: String,
    pub column: String,
    pub zoom_column: String,
    pub time_from: Option<f64>,
    pub time_to: Option<f64>,
    pub col_from: Option<f64>,
    pub col_to: Option<f64>,
}

// UFS ì¢…í•© í†µê³„ ë¶„ì„ì„ ìœ„í•œ ë§¤ê°œë³€ìˆ˜ êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
pub struct UfsAllStatsParams {
    pub logname: String,
    pub zoom_column: String,
    pub time_from: Option<f64>,
    pub time_to: Option<f64>,
    pub col_from: Option<f64>,
    pub col_to: Option<f64>,
}

// UFS ë ˆì´í„´ì‹œ í›„ì²˜ë¦¬ í•¨ìˆ˜
pub fn ufs_bottom_half_latency_process(mut ufs_list: Vec<UFS>) -> Vec<UFS> {
    // ì´ë²¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë²¡í„° ë°˜í™˜
    if ufs_list.is_empty() {
        return ufs_list;
    }

    // ì‹œì‘ ì‹œê°„ ê¸°ë¡
    let start_time = std::time::Instant::now();
    println!("\nğŸ”„ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“Š UFS Latency í›„ì²˜ë¦¬ ì‹œì‘");
    println!("   ì´ ì´ë²¤íŠ¸ ìˆ˜: {}", ufs_list.len());
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    
    // ì •ë ¬ ì—¬ë¶€ í™•ì¸ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìœ¼ë©´ ì •ë ¬ ìŠ¤í‚µ)
    println!("\n[1/2] â±ï¸  ë°ì´í„° ìˆœì„œ í™•ì¸ ì¤‘...");
    let sort_start = std::time::Instant::now();
    let mut needs_sort = false;
    for i in 1..ufs_list.len().min(1000) {
        if ufs_list[i - 1].time > ufs_list[i].time {
            needs_sort = true;
            break;
        }
    }
    
    let sort_elapsed = if needs_sort {
        println!("      âš ï¸  ì •ë ¬ë˜ì§€ ì•Šì€ ë°ì´í„° ê°ì§€, ì •ë ¬ ì¤‘...");
        ufs_list.sort_unstable_by(|a, b| {
            a.time.partial_cmp(&b.time).unwrap_or(std::cmp::Ordering::Equal)
        });
        let elapsed = sort_start.elapsed().as_secs_f64();
        println!("      âœ… ì •ë ¬ ì™„ë£Œ: {:.2}ì´ˆ", elapsed);
        elapsed
    } else {
        let elapsed = sort_start.elapsed().as_secs_f64();
        println!("      âœ… ì´ë¯¸ ì •ë ¬ë¨ (ì •ë ¬ ìŠ¤í‚µ): {:.3}ì´ˆ", elapsed);
        elapsed
    };

    // ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ìš©ëŸ‰ ìµœì í™” (ë” ì •í™•í•œ ì¶”ì •)
    let estimated_capacity = (ufs_list.len() / 4).max(1024);
    let mut req_times: HashMap<(u32, String), f64> = HashMap::with_capacity(estimated_capacity);
    
    let mut current_qd: u32 = 0;
    let mut last_complete_time: Option<f64> = None;
    let mut last_complete_qd0_time: Option<f64> = None;
    let mut first_c: bool = false;
    let mut first_complete_time: f64 = 0.0;

    // ì´ì „ send_reqì˜ ì •ë³´ë¥¼ ì €ì¥í•  ë³€ìˆ˜ë“¤
    let mut prev_send_req: Option<(u64, u32, String)> = None; // (lba, size, opcode)

    // í”„ë¡œê·¸ë ˆìŠ¤ ì¹´ìš´í„° ìµœì í™”
    let total_events = ufs_list.len();
    let report_threshold = total_events / 20; // 5% ê°„ê²© (ë” ì ì€ ì¶œë ¥)
    
    println!("\n[2/2] âš™ï¸  Latency ë° ì—°ì†ì„± ê³„ì‚° ì¤‘...");
    let processing_start = std::time::Instant::now();

    for (idx, ufs) in ufs_list.iter_mut().enumerate() {
        // ì§„í–‰ ìƒí™© ë³´ê³  (5% ê°„ê²©, ëª¨ë“ˆë¡œ ì—°ì‚° ì‚¬ìš©)
        if report_threshold > 0 && idx % report_threshold == 0 && idx > 0 {
            let progress = (idx * 100) / total_events;
            let elapsed = processing_start.elapsed().as_secs_f64();
            let rate = idx as f64 / elapsed;
            let remaining = total_events - idx;
            let eta = if rate > 0.0 { remaining as f64 / rate } else { 0.0 };
            println!("      ğŸ“Œ ì§„í–‰ë¥ : {}% ({}/{}) | ì†ë„: {:.0} events/s | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {:.1}ì´ˆ", 
                     progress, idx, total_events, rate, eta);
        }

        // ì„±ëŠ¥ ìµœì í™”: ë¬¸ìì—´ ë¹„êµë¥¼ ë°”ì´íŠ¸ ë¹„êµë¡œ ëŒ€ì²´
        let action_bytes = ufs.action.as_bytes();
        
        if action_bytes == b"send_req" {
            // ì—°ì†ì„± ì²´í¬: ì´ì „ send_reqê°€ ìˆëŠ” ê²½ìš°
            if let Some((prev_lba, prev_size, ref prev_opcode)) = prev_send_req {
                let prev_end_addr = prev_lba + prev_size as u64;
                // í˜„ì¬ ìš”ì²­ì˜ ì‹œì‘ ì£¼ì†Œê°€ ì´ì „ ìš”ì²­ì˜ ë ì£¼ì†Œì™€ ê°™ê³ , opcodeê°€ ê°™ì€ ê²½ìš°
                ufs.continuous = ufs.lba == prev_end_addr && ufs.opcode == *prev_opcode;
            } else {
                ufs.continuous = false;
            }

            // í˜„ì¬ send_req ì •ë³´ ì €ì¥ (clone ìµœì†Œí™”)
            prev_send_req = Some((ufs.lba, ufs.size, ufs.opcode.clone()));

            req_times.insert((ufs.tag, ufs.opcode.clone()), ufs.time);
            current_qd += 1;
            if current_qd == 1 {
                if let Some(t) = last_complete_qd0_time {
                    ufs.ctod = (ufs.time - t) * MILLISECONDS as f64;
                }
                first_c = true;
                first_complete_time = ufs.time;
            }
        } else if action_bytes == b"complete_rsp" {
            // complete_rspëŠ” continuous ì²´í¬í•˜ì§€ ì•ŠìŒ
            ufs.continuous = false;

            current_qd = current_qd.saturating_sub(1);
            if let Some(send_time) = req_times.remove(&(ufs.tag, ufs.opcode.clone())) {
                ufs.dtoc = (ufs.time - send_time) * MILLISECONDS as f64;
            }
            
            // ì¡°ê±´ ë¶„ê¸° ìµœì í™”
            if first_c {
                ufs.ctoc = (ufs.time - first_complete_time) * MILLISECONDS as f64;
                first_c = false;
            } else if let Some(t) = last_complete_time {
                ufs.ctoc = (ufs.time - t) * MILLISECONDS as f64;
            }
            
            if current_qd == 0 {
                last_complete_qd0_time = Some(ufs.time);
            }
            last_complete_time = Some(ufs.time);
        } else {
            ufs.continuous = false;
        }
        ufs.qd = current_qd;
    }

    let processing_elapsed = processing_start.elapsed().as_secs_f64();
    let processing_rate = ufs_list.len() as f64 / processing_elapsed;
    println!("      âœ… ê³„ì‚° ì™„ë£Œ: {} ì´ë²¤íŠ¸ | {:.2}ì´ˆ | {:.0} events/s", 
             ufs_list.len(), processing_elapsed, processing_rate);
    
    // ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ ë²¡í„° í¬ê¸° ì¡°ì •
    ufs_list.shrink_to_fit();

    let total_elapsed = start_time.elapsed().as_secs_f64();
    let total_rate = ufs_list.len() as f64 / total_elapsed;
    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("âœ¨ UFS Latency í›„ì²˜ë¦¬ ì™„ë£Œ!");
    println!("   ì´ ì†Œìš” ì‹œê°„: {:.2}ì´ˆ", total_elapsed);
    println!("   í‰ê·  ì²˜ë¦¬ ì†ë„: {:.0} events/s", total_rate);
    println!("   ìµœì¢… ì´ë²¤íŠ¸ ìˆ˜: {}", ufs_list.len());
    println!("   ë‹¨ê³„ë³„ ì‹œê°„:");
    println!("     - ì •ë ¬: {:.2}ì´ˆ ({:.1}%)", sort_elapsed, (sort_elapsed / total_elapsed) * 100.0);
    println!("     - Latency ê³„ì‚°: {:.2}ì´ˆ ({:.1}%)", processing_elapsed, (processing_elapsed / total_elapsed) * 100.0);
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");
    
    ufs_list
}

// Vec<UFS>ë¥¼ Arrow RecordBatchë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
pub fn ufs_to_record_batch(ufs_list: &[UFS]) -> Result<RecordBatch, String> {
    // ê° í•„ë“œë³„ë¡œ Arrow ë°°ì—´ ìƒì„±
    let time_array = Float64Array::from(ufs_list.iter().map(|u| u.time).collect::<Vec<f64>>());
    let process_array = StringArray::from(
        ufs_list
            .iter()
            .map(|u| u.process.clone())
            .collect::<Vec<String>>(),
    );
    let cpu_array = UInt32Array::from(ufs_list.iter().map(|u| u.cpu).collect::<Vec<u32>>());
    let action_array = StringArray::from(
        ufs_list
            .iter()
            .map(|u| u.action.clone())
            .collect::<Vec<String>>(),
    );
    let tag_array = UInt32Array::from(ufs_list.iter().map(|u| u.tag).collect::<Vec<u32>>());
    let opcode_array = StringArray::from(
        ufs_list
            .iter()
            .map(|u| u.opcode.clone())
            .collect::<Vec<String>>(),
    );
    let lba_array = UInt64Array::from(ufs_list.iter().map(|u| u.lba).collect::<Vec<u64>>());
    let size_array = UInt32Array::from(ufs_list.iter().map(|u| u.size).collect::<Vec<u32>>());
    let groupid_array = UInt32Array::from(ufs_list.iter().map(|u| u.groupid).collect::<Vec<u32>>());
    let hwqid_array = UInt32Array::from(ufs_list.iter().map(|u| u.hwqid).collect::<Vec<u32>>());
    let qd_array = UInt32Array::from(ufs_list.iter().map(|u| u.qd).collect::<Vec<u32>>());
    let dtoc_array = Float64Array::from(ufs_list.iter().map(|u| u.dtoc).collect::<Vec<f64>>());
    let ctoc_array = Float64Array::from(ufs_list.iter().map(|u| u.ctoc).collect::<Vec<f64>>());
    let ctod_array = Float64Array::from(ufs_list.iter().map(|u| u.ctod).collect::<Vec<f64>>());
    let continues_array =
        BooleanArray::from(ufs_list.iter().map(|u| u.continuous).collect::<Vec<bool>>());

    // ìŠ¤í‚¤ë§ˆ ì •ì˜
    let schema = Arc::new(Schema::new(vec![
        Field::new("time", DataType::Float64, false),
        Field::new("process", DataType::Utf8, false),
        Field::new("cpu", DataType::UInt32, false),
        Field::new("action", DataType::Utf8, false),
        Field::new("tag", DataType::UInt32, false),
        Field::new("opcode", DataType::Utf8, false),
        Field::new("lba", DataType::UInt64, false),
        Field::new("size", DataType::UInt32, false),
        Field::new("groupid", DataType::UInt32, false),
        Field::new("hwqid", DataType::UInt32, false),
        Field::new("qd", DataType::UInt32, false),
        Field::new("dtoc", DataType::Float64, false),
        Field::new("ctoc", DataType::Float64, false),
        Field::new("ctod", DataType::Float64, false),
        Field::new("continuous", DataType::Boolean, false),
    ]));

    // RecordBatch ìƒì„±
    RecordBatch::try_new(
        schema,
        vec![
            Arc::new(time_array) as ArrayRef,
            Arc::new(process_array) as ArrayRef,
            Arc::new(cpu_array) as ArrayRef,
            Arc::new(action_array) as ArrayRef,
            Arc::new(tag_array) as ArrayRef,
            Arc::new(opcode_array) as ArrayRef,
            Arc::new(lba_array) as ArrayRef,
            Arc::new(size_array) as ArrayRef,
            Arc::new(groupid_array) as ArrayRef,
            Arc::new(hwqid_array) as ArrayRef,
            Arc::new(qd_array) as ArrayRef,
            Arc::new(dtoc_array) as ArrayRef,
            Arc::new(ctoc_array) as ArrayRef,
            Arc::new(ctod_array) as ArrayRef,
            Arc::new(continues_array) as ArrayRef,
        ],
    )
    .map_err(|e| e.to_string())
}

// Parquet íŒŒì¼ ì €ì¥ í•¨ìˆ˜ - chunk ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ OOM ë°©ì§€
pub fn save_ufs_to_parquet(
    ufs_list: &[UFS],
    logfolder: String,
    fname: String,
    timestamp: &str,
    window: Option<&tauri::Window>,
) -> Result<String, String> {
    // logfolder ë‚´ì— stem í´ë” ìƒì„±
    let stem = PathBuf::from(&fname)
        .file_stem()
        .ok_or("Invalid filename")?
        .to_string_lossy()
        .to_string();

    let mut folder_path = PathBuf::from(logfolder);
    folder_path.push(&stem);
    create_dir_all(&folder_path).map_err(|e| e.to_string())?;

    let ufs_filename = format!("{}_ufs.parquet", timestamp);
    let mut path = folder_path;
    path.push(&ufs_filename);

    // chunk í¬ê¸° ì„¤ì • (100,000 ë ˆì½”ë“œì”© ì²˜ë¦¬)
    const CHUNK_SIZE: usize = 400_000;
    let total_records = ufs_list.len();
    
    if total_records == 0 {
        return Err("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.".to_string());
    }
    
    println!("UFS ë°ì´í„° ì €ì¥ ì‹œì‘: {} ë ˆì½”ë“œë¥¼ {} ë ˆì½”ë“œì”© Chunkë¡œ ì²˜ë¦¬", total_records, CHUNK_SIZE);
    
    let total_chunks = (total_records + CHUNK_SIZE - 1) / CHUNK_SIZE;
    
    // ì²« ë²ˆì§¸ Chunkë¡œ ìŠ¤í‚¤ë§ˆ ìƒì„±
    let first_chunk = if total_records > CHUNK_SIZE {
        &ufs_list[0..CHUNK_SIZE]
    } else {
        ufs_list
    };
    
    let first_batch = ufs_to_record_batch(first_chunk)?;
    let schema = first_batch.schema();
    let file = File::create(&path).map_err(|e| e.to_string())?;
    let mut writer = ArrowWriter::try_new(file, schema.clone(), None).map_err(|e| e.to_string())?;
    
        // ì²« ë²ˆì§¸ Chunk ì“°ê¸°
    writer.write(&first_batch).map_err(|e| e.to_string())?;
    println!("UFS Chunk 1/{} ì €ì¥ ì™„ë£Œ", total_chunks);
    
    // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì²« ë²ˆì§¸ Chunk)
    if let Some(w) = window {
        let progress = 85.0 + (1.0 / total_chunks as f64) * 10.0; // 85%ì—ì„œ 95% ì‚¬ì´
        let _ = w.emit("trace-progress", crate::trace::ProgressEvent {
            stage: "saving".to_string(),
            progress: progress as f32,
            current: (85 + ((1 * 10) / total_chunks)) as u64,
            total: 100,
            message: format!("UFS Parquet ì €ì¥ ì¤‘: {}/{} Chunk", 1, total_chunks),
            eta_seconds: (total_chunks - 1) as f32 * 0.5,
            processing_speed: 0.0,
        });
    }
    
    // ë‚˜ë¨¸ì§€ Chunkë“¤ ì²˜ë¦¬
    let mut chunk_num = 2;
    for chunk_start in (CHUNK_SIZE..total_records).step_by(CHUNK_SIZE) {
        let chunk_end = std::cmp::min(chunk_start + CHUNK_SIZE, total_records);
        let chunk = &ufs_list[chunk_start..chunk_end];
        
        let batch = ufs_to_record_batch(chunk)?;
        writer.write(&batch).map_err(|e| e.to_string())?;
        
        println!("UFS Chunk {}/{} ì €ì¥ ì™„ë£Œ", chunk_num, total_chunks);
        
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if let Some(w) = window {
            let progress = 85.0 + (chunk_num as f64 / total_chunks as f64) * 10.0;
            let _ = w.emit("trace-progress", crate::trace::ProgressEvent {
                stage: "saving".to_string(),
                progress: progress as f32,
                current: (85 + ((chunk_num * 10) / total_chunks)) as u64,
                total: 100,
                message: format!("UFS Parquet ì €ì¥ ì¤‘: {}/{} Chunk", chunk_num, total_chunks),
                eta_seconds: (total_chunks - chunk_num) as f32 * 0.5,
                processing_speed: 0.0,
            });
        }
        
        chunk_num += 1;
    }
    
    writer.close().map_err(|e| e.to_string())?;
    println!("UFS Parquet íŒŒì¼ ì €ì¥ ì™„ë£Œ: {}", path.to_string_lossy());

    Ok(path.to_string_lossy().to_string())
}

// UFS ë ˆì´í„´ì‹œ í†µê³„ í•¨ìˆ˜
pub async fn latencystats(params: UfsLatencyStatsParams) -> Result<Vec<u8>, String> {
    // ë¬¸ìì—´ thresholdsë¥¼ ë°€ë¦¬ì´ˆ ê°’ìœ¼ë¡œ ë³€í™˜
    let mut threshold_values: Vec<f64> = Vec::new();
    for t in &params.thresholds {
        let ms = parse_time_to_ms(t)?;
        threshold_values.push(ms);
    }

    // í•„í„°ë§ ì ìš©
    let filtered_ufs =
        filter_ufs_data(&params.logname, params.time_from, params.time_to, &params.zoom_column, params.col_from, params.col_to, None)?;

    // LatencyStat ìƒì„± - columnì— ë”°ë¼ ë°ì´í„° ë§¤í•‘
    let latency_stats = match params.column.as_str() {
        "dtoc" | "ctoc" => filtered_ufs
            .iter()
            .filter(|ufs| ufs.action == "complete_rsp")
            .map(|ufs| LatencyStat {
                time: ufs.time,
                opcode: ufs.opcode.clone(),
                value: if params.column == "dtoc" {
                    LatencyValue::F64(ufs.dtoc)
                } else {
                    LatencyValue::F64(ufs.ctoc)
                },
            })
            .collect::<Vec<_>>(),
        "ctod" => filtered_ufs
            .iter()
            .filter(|ufs| ufs.action == "send_req")
            .map(|ufs| LatencyStat {
                time: ufs.time,
                opcode: ufs.opcode.clone(),
                value: LatencyValue::F64(ufs.ctod),
            })
            .collect::<Vec<_>>(),
        _ => return Err(format!("Invalid column: {}", params.column)),
    };

    // ì´ë¯¸ parquetì—ì„œ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì •ë ¬ ë¶ˆí•„ìš”

    // ê° opcodeë³„ ë ˆì´í„´ì‹œ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
    let mut latency_counts = std::collections::BTreeMap::new();
    let opcodes: std::collections::HashSet<String> = latency_stats
        .iter()
        .map(|stat| stat.opcode.clone())
        .collect();

    for opcode in opcodes {
        latency_counts.insert(opcode.clone(), initialize_ranges(&params.thresholds));
    }

    // ê° ë°ì´í„°ì˜ latencyì— ë”°ë¼ êµ¬ê°„ ì¹´ìš´íŠ¸ ì¦ê°€
    for stat in &latency_stats {
        let latency = stat.value.as_f64();
        let range_key = create_range_key(latency, &threshold_values, &params.thresholds);

        if let Some(opcode_counts) = latency_counts.get_mut(&stat.opcode) {
            if let Some(count) = opcode_counts.get_mut(&range_key) {
                *count += 1;
            }
        }
    }

    // opcodeë³„ ê·¸ë£¹í•‘ í›„ í†µê³„ ê³„ì‚°
    let mut opcode_groups = std::collections::BTreeMap::new();
    for stat in &latency_stats {
        opcode_groups
            .entry(stat.opcode.clone())
            .or_insert_with(Vec::new)
            .push(stat.value.as_f64());
    }

    // ê° opcodeë³„ í†µê³„ ê³„ì‚°
    let mut summary_map = std::collections::BTreeMap::new();
    for (opcode, mut values) in opcode_groups {
        let summary = calculate_statistics(&mut values);
        summary_map.insert(opcode, summary);
    }

    let result = LatencyStats {
        latency_counts,
        summary: Some(summary_map),
    };

    serde_json::to_vec(&result).map_err(|e| e.to_string())
}

// UFS í¬ê¸° í†µê³„ í•¨ìˆ˜
pub async fn sizestats(params: UfsSizeStatsParams) -> Result<Vec<u8>, String> {
    // í•„í„°ë§ ì ìš©
    let filtered_ufs =
        filter_ufs_data(&params.logname, params.time_from, params.time_to, &params.zoom_column, params.col_from, params.col_to, None)?;

    // column ì¡°ê±´ì— ë”°ë¼ ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
    let filtered_ufs: Vec<&UFS> = filtered_ufs
        .iter()
        .filter(|ufs| match params.column.as_str() {
            "dtoc" | "ctoc" => ufs.action == "complete_rsp",
            "ctod" => ufs.action == "send_req",
            _ => false,
        })
        .collect();

    // opcodeë³„ í†µê³„ ì´ˆê¸°í™”
    let mut opcode_stats: std::collections::BTreeMap<String, std::collections::BTreeMap<u32, usize>> =
        std::collections::BTreeMap::new();
    let mut total_counts: std::collections::BTreeMap<String, usize> = std::collections::BTreeMap::new();

    // ëª¨ë“  opcode ìˆ˜ì§‘
    let opcodes: Vec<String> = filtered_ufs
        .iter()
        .map(|ufs| ufs.opcode.clone())
        .collect::<std::collections::HashSet<_>>()
        .into_iter()
        .collect();

    for opcode in &opcodes {
        opcode_stats.insert(opcode.clone(), std::collections::BTreeMap::new());
        total_counts.insert(opcode.clone(), 0);
    }

    // size ê¸°ì¤€ count ê³„ì‚°
    for ufs in &filtered_ufs {
        if let Some(size_counts) = opcode_stats.get_mut(&ufs.opcode) {
            let size_kb = ufs.size;

            *size_counts.entry(size_kb).or_insert(0) += 1;
            *total_counts.get_mut(&ufs.opcode).unwrap() += 1;
        }
    }

    // ì‘ë‹µ ê°ì²´ ìƒì„±
    let result = SizeStats {
        opcode_stats,
        total_counts,
    };

    serde_json::to_vec(&result).map_err(|e| e.to_string())
}

// UFS ì—°ì†ì„± í†µê³„ í•¨ìˆ˜
pub async fn continuity_stats(
    logname: String,
    zoom_column: String,
    time_from: Option<f64>,
    time_to: Option<f64>,
    col_from: Option<f64>,
    col_to: Option<f64>,
) -> Result<Vec<u8>, String> {
    // í•„í„°ë§ ì ìš©
    let filtered_ufs =
        filter_ufs_data(&logname, time_from, time_to, &zoom_column, col_from, col_to, None)?;

    // send_req ë™ì‘ë§Œ í•„í„°ë§ (ì—°ì†ì„±ì€ send_reqì—ì„œë§Œ ì˜ë¯¸ ìˆìŒ)
    // ì£¼ë¡œ ê´€ì‹¬ ìˆëŠ” opcodeë§Œ í•„í„°ë§: 0x28(read), 0x2a(write)
    let send_reqs: Vec<&UFS> = filtered_ufs
        .iter()
        .filter(|ufs| {
            ufs.action == "send_req"
                && (ufs.opcode == "0x28" || ufs.opcode == "0x2a" || ufs.opcode == "0x42")
        })
        .collect();

    // opcodeë³„ ì—°ì†ì„± í†µê³„ ìˆ˜ì§‘
    let mut op_stats: BTreeMap<String, ContinuityCount> = BTreeMap::new();
    let mut total_requests = 0;
    let mut total_continuous = 0;
    let mut total_bytes: u64 = 0;
    let mut continuous_bytes: u64 = 0;

    for ufs in &send_reqs {
        // opcodeë³„ í†µê³„ ì—…ë°ì´íŠ¸
        let stats = op_stats
            .entry(ufs.opcode.clone())
            .or_insert(ContinuityCount {
                continuous: 0,
                non_continuous: 0,
                ratio: 0.0,
                total_bytes: 0,
                continuous_bytes: 0,
                bytes_ratio: 0.0,
            });

        // UFSì˜ size í•„ë“œëŠ” ì´ë¯¸ 4KB ë‹¨ìœ„ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
        let bytes = ufs.size as u64 * 4096; // 4KB = 4096 bytes
        stats.total_bytes += bytes;
        total_bytes += bytes;

        if ufs.continuous {
            stats.continuous += 1;
            stats.continuous_bytes += bytes;
            total_continuous += 1;
            continuous_bytes += bytes;
        } else {
            stats.non_continuous += 1;
        }
        total_requests += 1;
    }

    // ë¹„ìœ¨ ê³„ì‚°
    for (_, stats) in op_stats.iter_mut() {
        let total = stats.continuous + stats.non_continuous;
        if total > 0 {
            stats.ratio = (stats.continuous as f64) / (total as f64) * 100.0;
            stats.bytes_ratio =
                (stats.continuous_bytes as f64) / (stats.total_bytes as f64) * 100.0;
        }
    }

    // ì „ì²´ í†µê³„ ê³„ì‚°
    let overall_ratio = if total_requests > 0 {
        (total_continuous as f64) / (total_requests as f64) * 100.0
    } else {
        0.0
    };

    let bytes_ratio = if total_bytes > 0 {
        (continuous_bytes as f64) / (total_bytes as f64) * 100.0
    } else {
        0.0
    };

    let result = ContinuityStats {
        op_stats,
        total: TotalContinuity {
            total_requests,
            continuous_requests: total_continuous,
            overall_ratio,
            total_bytes,
            continuous_bytes,
            bytes_ratio,
        },
    };

    serde_json::to_vec(&result).map_err(|e| e.to_string())
}

// UFS ì „ì²´ í†µê³„ ê³„ì‚° í•¨ìˆ˜ - ë‹¨ì¼ í•„í„°ë§ìœ¼ë¡œ ëª¨ë“  í†µê³„ ê³„ì‚°
pub async fn allstats(params: UfsAllStatsParams, thresholds: Vec<String>) -> Result<Vec<u8>, String> {
    // ë¬¸ìì—´ thresholdë¥¼ ë°€ë¦¬ì´ˆ ê°’ìœ¼ë¡œ ë³€í™˜
    let mut threshold_values: Vec<f64> = Vec::new();
    for t in &thresholds {
        let ms = parse_time_to_ms(t)?;
        threshold_values.push(ms);
    }

    // í•„í„°ë§ ì ìš©
    let filtered_ufs =
        filter_ufs_data(&params.logname, params.time_from, params.time_to, &params.zoom_column, params.col_from, params.col_to, None)?;

    // ëª¨ë“  opcode ìˆ˜ì§‘
    let opcodes: Vec<String> = filtered_ufs
        .iter()
        .map(|ufs| ufs.opcode.clone())
        .collect::<std::collections::HashSet<_>>()
        .into_iter()
        .collect();

    // í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
    let mut dtoc_counts = std::collections::BTreeMap::new();
    let mut ctod_counts = std::collections::BTreeMap::new();
    let mut ctoc_counts = std::collections::BTreeMap::new();
    let mut dtoc_groups = std::collections::BTreeMap::new();
    let mut ctod_groups = std::collections::BTreeMap::new();
    let mut ctoc_groups = std::collections::BTreeMap::new();

    let mut size_stats = std::collections::BTreeMap::new();
    let mut total_counts = std::collections::BTreeMap::new();

    let mut opcode_qd = std::collections::BTreeMap::new();

    // ì´ˆê¸°í™”
    for opcode in &opcodes {
        dtoc_counts.insert(opcode.clone(), initialize_ranges(&thresholds));
        ctod_counts.insert(opcode.clone(), initialize_ranges(&thresholds));
        ctoc_counts.insert(opcode.clone(), initialize_ranges(&thresholds));
        dtoc_groups.insert(opcode.clone(), Vec::new());
        ctod_groups.insert(opcode.clone(), Vec::new());
        ctoc_groups.insert(opcode.clone(), Vec::new());
        size_stats.insert(opcode.clone(), std::collections::BTreeMap::new());
        total_counts.insert(opcode.clone(), 0);
        opcode_qd.insert(opcode.clone(), Vec::new());
    }

    // ì—°ì†ì„± í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
    let mut op_stats: BTreeMap<String, ContinuityCount> = BTreeMap::new();
    let mut total_requests = 0;
    let mut total_continuous = 0;
    let mut total_bytes_continuity: u64 = 0;
    let mut continuous_bytes: u64 = 0;

    // ì „ì²´ í†µê³„ í•œë²ˆì— ê³„ì‚°
    for ufs in &filtered_ufs {
        if ufs.action == "complete_rsp" {
            // DTOC ë ˆì´í„´ì‹œ í†µê³„
            let range_key = create_range_key(ufs.dtoc, &threshold_values, &thresholds);
            if let Some(counts) = dtoc_counts.get_mut(&ufs.opcode) {
                if let Some(count) = counts.get_mut(&range_key) {
                    *count += 1;
                }
            }
            dtoc_groups.entry(ufs.opcode.clone()).or_default().push(ufs.dtoc);

            // CTOC ë ˆì´í„´ì‹œ í†µê³„
            let range_key = create_range_key(ufs.ctoc, &threshold_values, &thresholds);
            if let Some(counts) = ctoc_counts.get_mut(&ufs.opcode) {
                if let Some(count) = counts.get_mut(&range_key) {
                    *count += 1;
                }
            }
            ctoc_groups.entry(ufs.opcode.clone()).or_default().push(ufs.ctoc);

            // QD í†µê³„
            opcode_qd.entry(ufs.opcode.clone()).or_default().push(ufs.qd as f64);
        }

        if ufs.action == "send_req" {
            // CTOD ë ˆì´í„´ì‹œ í†µê³„
            let range_key = create_range_key(ufs.ctod, &threshold_values, &thresholds);
            if let Some(counts) = ctod_counts.get_mut(&ufs.opcode) {
                if let Some(count) = counts.get_mut(&range_key) {
                    *count += 1;
                }
            }
            ctod_groups.entry(ufs.opcode.clone()).or_default().push(ufs.ctod);

            // ì—°ì†ì„± í†µê³„ (send_reqì—ì„œë§Œ ì—°ì†ì„±ì´ ì˜ë¯¸ê°€ ìˆìŒ)
            if ufs.opcode == "0x28" || ufs.opcode == "0x2a" || ufs.opcode == "0x42" {
                // opcodeë³„ ì—°ì†ì„± í†µê³„ ì—…ë°ì´íŠ¸
                let stats = op_stats
                    .entry(ufs.opcode.clone())
                    .or_insert(ContinuityCount {
                        continuous: 0,
                        non_continuous: 0,
                        ratio: 0.0,
                        total_bytes: 0,
                        continuous_bytes: 0,
                        bytes_ratio: 0.0,
                    });

                // UFSì˜ size í•„ë“œëŠ” ì´ë¯¸ 4KB ë‹¨ìœ„ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
                let bytes = ufs.size as u64 * 4096; // 4KB = 4096 bytes
                stats.total_bytes += bytes;
                total_bytes_continuity += bytes;

                if ufs.continuous {
                    stats.continuous += 1;
                    stats.continuous_bytes += bytes;
                    total_continuous += 1;
                    continuous_bytes += bytes;
                } else {
                    stats.non_continuous += 1;
                }
                total_requests += 1;
            }
        }

        // í¬ê¸° í†µê³„ (KB ë‹¨ìœ„ë¡œ ë³€í™˜)
        let size_kb = ufs.size * 4; // 4KB ë‹¨ìœ„ì´ë¯€ë¡œ 4ë¥¼ ê³±í•¨
        if let Some(size_counts) = size_stats.get_mut(&ufs.opcode) {
            *size_counts.entry(size_kb).or_insert(0) += 1;
            *total_counts.get_mut(&ufs.opcode).unwrap() += 1;
        }
    }

    // ì—°ì†ì„± í†µê³„ì˜ ë¹„ìœ¨ ê³„ì‚°
    for (_, stats) in op_stats.iter_mut() {
        let total = stats.continuous + stats.non_continuous;
        if total > 0 {
            stats.ratio = (stats.continuous as f64) / (total as f64) * 100.0;
            stats.bytes_ratio =
                (stats.continuous_bytes as f64) / (stats.total_bytes as f64) * 100.0;
        }
    }

    // í†µê³„ ìš”ì•½ ê³„ì‚°
    let mut dtoc_summary = std::collections::BTreeMap::new();
    let mut ctod_summary = std::collections::BTreeMap::new();
    let mut ctoc_summary = std::collections::BTreeMap::new();
    let mut qd_summary = std::collections::BTreeMap::new();

    for (opcode, mut values) in dtoc_groups {
        dtoc_summary.insert(opcode, calculate_statistics(&mut values));
    }

    for (opcode, mut values) in ctod_groups {
        ctod_summary.insert(opcode, calculate_statistics(&mut values));
    }

    for (opcode, mut values) in ctoc_groups {
        ctoc_summary.insert(opcode, calculate_statistics(&mut values));
    }

    for (opcode, mut values) in opcode_qd {
        qd_summary.insert(opcode, calculate_statistics(&mut values));
    }

    // ê²°ê³¼ ê°ì²´ ìƒì„±
    let dtoc_stats = LatencyStats {
        latency_counts: dtoc_counts,
        summary: Some(dtoc_summary),
    };

    let ctod_stats = LatencyStats {
        latency_counts: ctod_counts,
        summary: Some(ctod_summary),
    };

    let ctoc_stats = LatencyStats {
        latency_counts: ctoc_counts,
        summary: Some(ctoc_summary),
    };

    let size_result = SizeStats {
        opcode_stats: size_stats,
        total_counts,
    };

    // ì „ì²´ ì—°ì†ì„± í†µê³„ ê³„ì‚°
    let overall_ratio = if total_requests > 0 {
        (total_continuous as f64) / (total_requests as f64) * 100.0
    } else {
        0.0
    };

    let bytes_ratio = if total_bytes_continuity > 0 {
        (continuous_bytes as f64) / (total_bytes_continuity as f64) * 100.0
    } else {
        0.0
    };

    // TraceStats êµ¬ì¡°ì²´ë¥¼ ì‚¬ìš© (UfsTraceStats ëŒ€ì‹ )
    let result = TraceStats {
        dtoc_stat: dtoc_stats,
        ctod_stat: ctod_stats,
        ctoc_stat: ctoc_stats,
        size_counts: size_result,
        continuity: ContinuityStats {
            op_stats,
            total: TotalContinuity {
                total_requests,
                continuous_requests: total_continuous,
                overall_ratio,
                total_bytes: total_bytes_continuity,
                continuous_bytes,
                bytes_ratio,
            },
        },
    };

    serde_json::to_vec(&result).map_err(|e| e.to_string())
}
